{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf7aa847",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T09:00:19.202597Z",
     "iopub.status.busy": "2025-04-28T09:00:19.202346Z",
     "iopub.status.idle": "2025-04-28T09:00:19.209211Z",
     "shell.execute_reply": "2025-04-28T09:00:19.208437Z"
    },
    "papermill": {
     "duration": 0.012399,
     "end_time": "2025-04-28T09:00:19.210351",
     "exception": false,
     "start_time": "2025-04-28T09:00:19.197952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a70f15e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T09:00:19.217031Z",
     "iopub.status.busy": "2025-04-28T09:00:19.216806Z",
     "iopub.status.idle": "2025-04-28T09:00:23.515990Z",
     "shell.execute_reply": "2025-04-28T09:00:23.515434Z"
    },
    "papermill": {
     "duration": 4.304041,
     "end_time": "2025-04-28T09:00:23.517412",
     "exception": false,
     "start_time": "2025-04-28T09:00:19.213371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def is_image_file(file_path):\n",
    "    # Common image file extensions\n",
    "    image_extensions = [\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\", \".tiff\", \".webp\"]\n",
    "    # Get file extension\n",
    "    ext = os.path.splitext(file_path)[-1].lower()\n",
    "    return ext in image_extensions\n",
    "\n",
    "\n",
    "def is_valid_part_format(s):\n",
    "    # Define the pattern\n",
    "    pattern = r\"^part([1-9]|1[0-4])$\"\n",
    "    # Match the string against the pattern\n",
    "    match = re.match(pattern, s)\n",
    "    return bool(match)\n",
    "\n",
    "\n",
    "class TripletDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir,\n",
    "        transform=None,\n",
    "        sample_negatives=\"epoch\",\n",
    "        limit=-1,\n",
    "        neg_only_reviews=True,\n",
    "        type='train'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        root_dir: Path to dataset (folders as classes)\n",
    "        transform: Image transformations (e.g., augmentation, normalization)\n",
    "        sample_negatives:\n",
    "            - \"batch\" â†’ Selects a random negative for each sample dynamically.\n",
    "            - \"epoch\" â†’ Assigns a negative at the start of each epoch.\n",
    "            - \"fixed\" â†’ Precomputed negative samples from a CSV file.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.sample_negatives = sample_negatives\n",
    "        self.neg_only_reviews = neg_only_reviews\n",
    "\n",
    "        self.class_to_images = defaultdict(list)  # { class: [image1, image2, ...] }\n",
    "        self.samples = []  # [(anchor_path, positive_path, class)]\n",
    "\n",
    "        # Read dataset structure\n",
    "        for part_folder in os.listdir(root_dir):\n",
    "            if self.__len__() >= limit and limit > 0:\n",
    "                break\n",
    "            if type == 'train':\n",
    "                part_path = os.path.join(root_dir, part_folder, part_folder)\n",
    "            else:\n",
    "                part_path = os.path.join(root_dir, part_folder)\n",
    "            print(part_path)\n",
    "            if not os.path.isdir(part_path) or not is_valid_part_format(part_folder):\n",
    "                continue\n",
    "            for product_folder in os.listdir(part_path):\n",
    "                if self.__len__() >= limit and limit > 0:\n",
    "                    break\n",
    "\n",
    "                product_path = os.path.join(part_path, product_folder)\n",
    "                if os.path.isdir(product_path):\n",
    "                    product_and_review = [\n",
    "                        os.path.join(product_path, img)\n",
    "                        for img in os.listdir(product_path)\n",
    "                    ]\n",
    "                    if (\n",
    "                        len(product_and_review) < 2\n",
    "                    ):  # Ensure at least an anchor-positive pair\n",
    "                        continue\n",
    "\n",
    "                    positive = None\n",
    "                    anchor = None\n",
    "                    for i in product_and_review:\n",
    "                        if os.path.isdir(i):  # review\n",
    "                            reviews = [\n",
    "                                os.path.join(i, review_img)\n",
    "                                for review_img in os.listdir(i)\n",
    "                            ]\n",
    "                            if len(reviews) == 0:\n",
    "                                continue\n",
    "                            # Get only first review image if it have multiple reivews\n",
    "                            valid_review_imgs = [\n",
    "                                file for file in reviews if is_image_file(file)\n",
    "                            ]\n",
    "                            if len(valid_review_imgs) > 0:\n",
    "                                positive = valid_review_imgs[0]\n",
    "                        elif is_image_file(i) and anchor is None:\n",
    "                            anchor = i\n",
    "\n",
    "                        if positive is not None and anchor is not None:\n",
    "                            self.class_to_images[product_folder] = [\n",
    "                                anchor,\n",
    "                                positive,\n",
    "                            ]\n",
    "                            self.samples.append(\n",
    "                                (anchor, positive, product_folder)\n",
    "                            )  # Anchor & positive\n",
    "                            continue\n",
    "\n",
    "        # Precompute negatives if needed\n",
    "        if self.sample_negatives == \"epoch\":\n",
    "            self.negative_map = self.assign_negatives()\n",
    "\n",
    "    def assign_negatives(self):\n",
    "        \"\"\"Assigns a random negative from a different class at the start of each epoch.\"\"\"\n",
    "        negative_map = {}\n",
    "        product_list = list(self.class_to_images.keys())\n",
    "\n",
    "        for product_label in self.class_to_images:\n",
    "            neg_reviews = [cls for cls in product_list if cls != product_label]\n",
    "            neg_review = random.choice(neg_reviews)\n",
    "            if not self.neg_only_reviews:\n",
    "                negative_map[product_label] = random.choice(\n",
    "                    self.class_to_images[neg_review]\n",
    "                )\n",
    "            else:\n",
    "                negative_map[product_label] = self.class_to_images[neg_review][1]\n",
    "\n",
    "        return negative_map\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        anchor_path, positive_path, product_label = self.samples[index]\n",
    "\n",
    "        # Choose negative based on sampling strategy\n",
    "        if self.sample_negatives == \"batch\":\n",
    "            neg_reviews = [\n",
    "                cls for cls in self.class_to_images.keys() if cls != product_label\n",
    "            ]\n",
    "            neg_review = random.choice(neg_reviews)\n",
    "            negative_path = random.choice(self.class_to_images[neg_review])\n",
    "        elif self.sample_negatives == \"epoch\":\n",
    "            negative_path = self.negative_map[product_label]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported sampling strategy. Use 'batch' or 'epoch'.\")\n",
    "\n",
    "        # Load images\n",
    "        anchor = Image.open(anchor_path).convert(\"RGB\")\n",
    "        positive = Image.open(positive_path).convert(\"RGB\")\n",
    "        # negative = Image.open(negative_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            anchor = self.transform(anchor)\n",
    "            positive = self.transform(positive)\n",
    "            # negative = self.transform(negative)\n",
    "\n",
    "        return anchor, positive\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def update_negatives(self):\n",
    "        \"\"\"Call this at the start of each epoch if using 'epoch' sampling.\"\"\"\n",
    "        if self.sample_negatives == \"epoch\":\n",
    "            self.negative_map = self.assign_negatives()\n",
    "\n",
    "\n",
    "\n",
    "class EvalTripletDataset(TripletDataset):\n",
    "\n",
    "    def assign_negatives(self):\n",
    "        \"\"\"\n",
    "        GÃ¡n cÃ¡c máº«u Ã¢m (negative samples) cho tá»«ng sáº£n pháº©m.\n",
    "\n",
    "        MÃ´ táº£:\n",
    "        - Duyá»‡t qua danh sÃ¡ch cÃ¡c sáº£n pháº©m (`class_to_images`).\n",
    "        - Má»—i sáº£n pháº©m Ä‘Æ°á»£c gÃ¡n vá»›i má»™t sáº£n pháº©m khÃ¡c lÃ m máº«u Ã¢m (negative).\n",
    "        - Sá»­ dá»¥ng ká»¹ thuáº­t xoay vÃ²ng (circular indexing) Ä‘á»ƒ Ä‘áº£m báº£o má»—i sáº£n pháº©m cÃ³ má»™t máº«u Ã¢m há»£p lá»‡.\n",
    "\n",
    "        Returns:\n",
    "            dict: Báº£n Ä‘á»“ Ã¡nh xáº¡ tá»« sáº£n pháº©m gá»‘c sang máº«u Ã¢m.\n",
    "        \"\"\"\n",
    "        negative_map = {}\n",
    "        product_list = list(self.class_to_images.keys())\n",
    "        total_products = len(product_list)\n",
    "\n",
    "        for idx, product_label in enumerate(self.class_to_images):\n",
    "            next_idx = (idx + 1) % total_products  # Xoay vÃ²ng danh sÃ¡ch\n",
    "            neg_reviews = product_list[next_idx]\n",
    "            negative_map[product_label] = self.class_to_images[neg_reviews][1]\n",
    "\n",
    "        return negative_map\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        anchor_path, positive_path, product_label = self.samples[index]\n",
    "\n",
    "        # Choose negative based on sampling strategy\n",
    "        if self.sample_negatives == \"batch\":\n",
    "            neg_reviews = [\n",
    "                cls for cls in self.class_to_images.keys() if cls != product_label\n",
    "            ]\n",
    "            neg_review = random.choice(neg_reviews)\n",
    "            negative_path = random.choice(self.class_to_images[neg_review])\n",
    "        elif self.sample_negatives == \"epoch\":\n",
    "            negative_path = self.negative_map[product_label]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported sampling strategy. Use 'batch' or 'epoch'.\")\n",
    "\n",
    "        # Load images\n",
    "        anchor = Image.open(anchor_path).convert(\"RGB\")\n",
    "        positive = Image.open(positive_path).convert(\"RGB\")\n",
    "        negative = Image.open(negative_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            anchor = self.transform(anchor)\n",
    "            positive = self.transform(positive)\n",
    "            negative = self.transform(negative)\n",
    "\n",
    "        return anchor, positive, negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a329349",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T09:00:23.524435Z",
     "iopub.status.busy": "2025-04-28T09:00:23.524028Z",
     "iopub.status.idle": "2025-04-28T09:00:28.169383Z",
     "shell.execute_reply": "2025-04-28T09:00:28.168802Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 4.650253,
     "end_time": "2025-04-28T09:00:28.170724",
     "exception": false,
     "start_time": "2025-04-28T09:00:23.520471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def test_image_embedding_model(\n",
    "    image1_path, image2_path, model, threshold, metric=\"l2\", show_images=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Kiá»ƒm tra má»©c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cá»§a hai áº£nh dá»±a trÃªn model embedding.\n",
    "\n",
    "    Args:\n",
    "        image1_path (str): ÄÆ°á»ng dáº«n áº£nh thá»© nháº¥t.\n",
    "        image2_path (str): ÄÆ°á»ng dáº«n áº£nh thá»© hai.\n",
    "        model (torch.nn.Module): MÃ´ hÃ¬nh trÃ­ch xuáº¥t embedding.\n",
    "        threshold (float): NgÆ°á»¡ng phÃ¢n biá»‡t giá»¯a áº£nh tÆ°Æ¡ng Ä‘á»“ng vÃ  khÃ´ng tÆ°Æ¡ng Ä‘á»“ng.\n",
    "        metric (str): PhÆ°Æ¡ng phÃ¡p Ä‘o lÆ°á»ng, chá»n 'cosine' hoáº·c 'l2'.\n",
    "        show_images (bool): Náº¿u True, hiá»ƒn thá»‹ hai áº£nh Ä‘á»ƒ so sÃ¡nh trá»±c quan.\n",
    "\n",
    "    Returns:\n",
    "        int: 1 náº¿u hai áº£nh tÆ°Æ¡ng Ä‘á»“ng, 0 náº¿u khÃ´ng tÆ°Æ¡ng Ä‘á»“ng.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # Tiá»n xá»­ lÃ½ áº£nh\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    def preprocess_image(image_path):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        return transform(image).unsqueeze(0)  # ThÃªm batch dimension\n",
    "\n",
    "    img1 = preprocess_image(image1_path)\n",
    "    img2 = preprocess_image(image2_path)\n",
    "    img1, img2 = img1.to(device), img2.to(device)\n",
    "    # Chuyá»ƒn áº£nh sang tensor vÃ  Ä‘Æ°a vÃ o model Ä‘á»ƒ láº¥y embedding\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        emb1 = model(img1).squeeze(0)  # (N, D) â†’ (D,)\n",
    "        emb2 = model(img2).squeeze(0)  # (N, D) â†’ (D,)\n",
    "\n",
    "    # TÃ­nh toÃ¡n khoáº£ng cÃ¡ch hoáº·c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng\n",
    "    if metric == \"cosine\":\n",
    "        similarity = F.cosine_similarity(emb1, emb2, dim=0).item()\n",
    "        score_text = f\"Similarity: {similarity:.4f}\"\n",
    "        result = 1 if similarity >= threshold else 0\n",
    "    elif metric == \"l2\":\n",
    "        distance = torch.norm(emb1 - emb2, p=2).item()\n",
    "        score_text = f\"Distance: {distance:.4f}\"\n",
    "        result = 1 if distance <= threshold else 0\n",
    "    else:\n",
    "        raise ValueError(\"metric pháº£i lÃ  'cosine' hoáº·c 'l2'\")\n",
    "\n",
    "    # Hiá»ƒn thá»‹ áº£nh náº¿u Ä‘Æ°á»£c yÃªu cáº§u\n",
    "    if show_images:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        axes[0].imshow(Image.open(image1_path))\n",
    "        axes[0].set_title(\"Image 1\")\n",
    "        axes[0].axis(\"off\")\n",
    "\n",
    "        axes[1].imshow(Image.open(image2_path))\n",
    "        axes[1].set_title(\"Image 2\")\n",
    "        axes[1].axis(\"off\")\n",
    "\n",
    "        plt.suptitle(score_text)\n",
    "        plt.show()\n",
    "\n",
    "    print(score_text)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def evaluate_batch(anchor, pos, neg, threshold=1.25, metric=\"l2\"):\n",
    "    \"\"\"\n",
    "    ÄÃ¡nh giÃ¡ theo batch vá»›i metric lÃ  cosine similarity hoáº·c L2 distance.\n",
    "\n",
    "    Args:\n",
    "        anchor (torch.Tensor): Batch embedding cá»§a anchor, shape (batch_size, embedding_dim)\n",
    "        pos (torch.Tensor): Batch embedding cá»§a positive, shape (batch_size, embedding_dim)\n",
    "        neg (torch.Tensor): Batch embedding cá»§a negative, shape (batch_size, embedding_dim)\n",
    "        threshold (float): NgÆ°á»¡ng quyáº¿t Ä‘á»‹nh máº«u cÃ³ giá»‘ng nhau khÃ´ng.\n",
    "        metric (str): 'cosine' hoáº·c 'l2' Ä‘á»ƒ chá»n phÆ°Æ¡ng phÃ¡p Ä‘o khoáº£ng cÃ¡ch.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (TP, TN, FP, FN)\n",
    "    \"\"\"\n",
    "\n",
    "    if metric == \"cosine\":\n",
    "        # TÃ­nh cosine similarity\n",
    "        sim_pos = F.cosine_similarity(\n",
    "            anchor, pos, dim=-1\n",
    "        )  # Cosine similarity giá»¯a anchor vÃ  positive\n",
    "        sim_neg = F.cosine_similarity(\n",
    "            anchor, neg, dim=-1\n",
    "        )  # Cosine similarity giá»¯a anchor vÃ  negative\n",
    "\n",
    "        # XÃ¡c Ä‘á»‹nh TP, FP, TN, FN\n",
    "        tp = (sim_pos >= threshold).sum().item()  # Dá»± Ä‘oÃ¡n Ä‘Ãºng positive\n",
    "        fn = (\n",
    "            (sim_pos < threshold).sum().item()\n",
    "        )  # Dá»± Ä‘oÃ¡n sai positive (Ä‘Ã¡ng láº½ giá»‘ng nhÆ°ng bá»‹ xem lÃ  khÃ¡c)\n",
    "        tn = (sim_neg < threshold).sum().item()  # Dá»± Ä‘oÃ¡n Ä‘Ãºng negative\n",
    "        fp = (\n",
    "            (sim_neg >= threshold).sum().item()\n",
    "        )  # Dá»± Ä‘oÃ¡n sai negative (Ä‘Ã¡ng láº½ khÃ¡c nhÆ°ng bá»‹ xem lÃ  giá»‘ng)\n",
    "\n",
    "    elif metric == \"l2\":\n",
    "        # TÃ­nh L2 distance\n",
    "        dist_pos = torch.norm(\n",
    "            anchor - pos, p=2, dim=-1\n",
    "        )  # Khoáº£ng cÃ¡ch L2 giá»¯a anchor vÃ  positive\n",
    "        dist_neg = torch.norm(\n",
    "            anchor - neg, p=2, dim=-1\n",
    "        )  # Khoáº£ng cÃ¡ch L2 giá»¯a anchor vÃ  negative\n",
    "\n",
    "        # XÃ¡c Ä‘á»‹nh TP, FP, TN, FN\n",
    "        tp = (dist_pos <= threshold).sum().item()  # Dá»± Ä‘oÃ¡n Ä‘Ãºng positive\n",
    "        fn = (dist_pos > threshold).sum().item()  # Dá»± Ä‘oÃ¡n sai positive\n",
    "        tn = (dist_neg > threshold).sum().item()  # Dá»± Ä‘oÃ¡n Ä‘Ãºng negative\n",
    "        fp = (dist_neg <= threshold).sum().item()  # Dá»± Ä‘oÃ¡n sai negative\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Metric must be 'cosine' or 'l2'\")\n",
    "\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "\n",
    "def evaluate_metrics(tp, tn, fp, fn):\n",
    "    # Accuracy\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "    # Precision\n",
    "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "\n",
    "    # Recall\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "\n",
    "    # F1-Score\n",
    "    f1_score = (\n",
    "        2 * (precision * recall) / (precision + recall)\n",
    "        if (precision + recall) != 0\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    # False Positive Rate (FPR)\n",
    "    fpr = fp / (fp + tn) if (fp + tn) != 0 else 0\n",
    "\n",
    "    # False Negative Rate (FNR)\n",
    "    fnr = fn / (fn + tp) if (fn + tp) != 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1_score,\n",
    "        \"False Positive Rate (FPR)\": fpr,\n",
    "        \"False Negative Rate (FNR)\": fnr,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5872dd67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T09:00:28.177658Z",
     "iopub.status.busy": "2025-04-28T09:00:28.177390Z",
     "iopub.status.idle": "2025-04-28T09:00:30.641668Z",
     "shell.execute_reply": "2025-04-28T09:00:30.641085Z"
    },
    "papermill": {
     "duration": 2.469285,
     "end_time": "2025-04-28T09:00:30.642981",
     "exception": false,
     "start_time": "2025-04-28T09:00:28.173696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "class SpamDetectorTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        criterion: torch.nn.Module,\n",
    "        train_loader: torch.utils.data.DataLoader,\n",
    "        valid_loader: torch.utils.data.DataLoader,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        scheduler=None,\n",
    "        lr_types: Literal[\"step\", \"epoch\"] = \"step\",\n",
    "        device: torch.device = \"cuda\",\n",
    "        epochs: int = 10,\n",
    "        max_norm: float = 0,\n",
    "        log_writer: wandb = None,\n",
    "        patience=3,\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.criterion = criterion\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.lr_types = lr_types\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.max_norm = max_norm\n",
    "        self.log_writer = log_writer\n",
    "        self.patience = patience\n",
    "\n",
    "    def train_one_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        global_step = epoch * len(self.train_loader)  # Global step trackingðŸ”¥\n",
    "\n",
    "        for step, (anchor, positive) in enumerate(self.train_loader):\n",
    "            anchor, positive = (\n",
    "                anchor.to(self.device),\n",
    "                positive.to(self.device),\n",
    "            )\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            anchor_embeded = self.model(anchor)\n",
    "            positive_embeded = self.model(positive)\n",
    "            loss = self.criterion(anchor_embeded, positive_embeded)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # TÃ­nh toÃ¡n Gradient Norm\n",
    "            total_norm = 0\n",
    "            param_count = 0\n",
    "\n",
    "            for param in self.model.parameters():\n",
    "                if param.grad is not None:\n",
    "                    param_norm = param.grad.norm().item()\n",
    "                    total_norm += param_norm\n",
    "                    param_count += 1\n",
    "\n",
    "            mean_grad_norm = total_norm / param_count if param_count > 0 else 0\n",
    "\n",
    "            # Ãp dá»¥ng Gradient Clipping náº¿u cáº§n\n",
    "            if self.max_norm > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_norm)\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Step scheduler per step\n",
    "            if self.scheduler and self.lr_types == \"step\":\n",
    "                self.scheduler.step()\n",
    "\n",
    "            # Log to W&B\n",
    "            if self.log_writer:\n",
    "                self.log_writer.log(\n",
    "                    {\n",
    "                        \"train/loss\": loss.item(),\n",
    "                        \"train/learning_rate\": self.optimizer.param_groups[0][\"lr\"],\n",
    "                        \"train/grad_norm\": mean_grad_norm,\n",
    "                        \"train/global_step\": global_step + step,\n",
    "                        \"train/epoch\": epoch + step / len(self.train_loader),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        epoch_loss = total_loss / len(self.train_loader)\n",
    "        if self.log_writer:\n",
    "            self.log_writer.log({\"train/mean_loss\": epoch_loss})\n",
    "\n",
    "        # Step scheduler per epoch\n",
    "        if self.scheduler and self.lr_types == \"epoch\":\n",
    "            self.scheduler.step()  # Step based on epoch\n",
    "\n",
    "        return epoch_loss\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        tp = 0\n",
    "        tn = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for step, (anchor, positive, negative) in enumerate(self.valid_loader):\n",
    "                anchor, positive, negative = (\n",
    "                    anchor.to(self.device),\n",
    "                    positive.to(self.device),\n",
    "                    negative.to(self.device),\n",
    "                )\n",
    "                anchor_embeded = self.model(anchor)\n",
    "                positive_embeded = self.model(positive)\n",
    "                negative_embeded = self.model(negative)\n",
    "                loss = self.criterion(\n",
    "                    anchor_embeded, positive_embeded\n",
    "                )\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                eval_step_result = evaluate_batch(\n",
    "                    anchor_embeded, positive_embeded, negative_embeded\n",
    "                )\n",
    "                tp += eval_step_result[0]\n",
    "                tn += eval_step_result[1]\n",
    "                fp += eval_step_result[2]\n",
    "                fn += eval_step_result[3]\n",
    "\n",
    "        metrics = evaluate_metrics(tp, tn, fp, fn)\n",
    "\n",
    "        # Calculate average loss\n",
    "        avg_loss = running_loss / len(self.valid_loader)\n",
    "\n",
    "        # Log metrics and loss to W&B\n",
    "        if self.log_writer:\n",
    "            # Log evaluation metrics\n",
    "            self.log_writer.log(\n",
    "                {\n",
    "                    \"val/accuracy\": metrics[\"Accuracy\"],\n",
    "                    \"val/precision\": metrics[\"Precision\"],\n",
    "                    \"val/recall\": metrics[\"Recall\"],\n",
    "                    \"val/f1_score\": metrics[\"F1-Score\"],\n",
    "                    \"val/False Positive Rate\": metrics[\"False Positive Rate (FPR)\"],\n",
    "                    \"val/False Negative Rate\": metrics[\"False Negative Rate (FNR)\"],\n",
    "                    \"val/val_loss\": avg_loss,\n",
    "                }\n",
    "            )\n",
    "        return avg_loss\n",
    "\n",
    "    def train(self, resume_from_checkpoint=None):\n",
    "        start_epoch = 0\n",
    "        best_val_loss = float(\"inf\")\n",
    "        epochs_without_improvement = 0  # Track epochs without improvement\n",
    "\n",
    "        # Load checkpoint if provided\n",
    "        if resume_from_checkpoint:\n",
    "            checkpoint = torch.load(resume_from_checkpoint, map_location=self.device)\n",
    "            self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "            self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "            if \"scheduler_state_dict\" in checkpoint and self.scheduler:\n",
    "                self.scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "\n",
    "            start_epoch = checkpoint[\"epoch\"] + 1\n",
    "            best_val_loss = checkpoint.get(\"best_val_loss\", float(\"inf\"))\n",
    "            epochs_without_improvement = checkpoint.get(\"epochs_without_improvement\", 0)\n",
    "\n",
    "            print(f\"Resuming training from epoch {start_epoch}\")\n",
    "        else:\n",
    "            print(\"Start training!\")\n",
    "        for epoch in range(start_epoch, start_epoch + self.epochs):\n",
    "            epoch_loss = self.train_one_epoch(epoch)\n",
    "            val_loss = self.validate()\n",
    "\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{self.epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\"\n",
    "            )\n",
    "\n",
    "            # Check if validation loss improved\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_without_improvement = 0  # Reset counter\n",
    "                print(\n",
    "                    f\"New best validation loss: {best_val_loss:.4f}. Saving checkpoint.\"\n",
    "                )\n",
    "\n",
    "                checkpoint = {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": self.model.state_dict(),\n",
    "                    \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "                    \"scheduler_state_dict\": (\n",
    "                        self.scheduler.state_dict() if self.scheduler else None\n",
    "                    ),\n",
    "                    \"best_val_loss\": best_val_loss,\n",
    "                    \"epochs_without_improvement\": epochs_without_improvement,\n",
    "                }\n",
    "                torch.save(checkpoint, f\"checkpoint_{epoch}.pth\")\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                print(\n",
    "                    f\"No improvement for {epochs_without_improvement}/{self.patience} epochs.\"\n",
    "                )\n",
    "\n",
    "            # Stop training if no improvement for `self.patience` epochs\n",
    "            if epochs_without_improvement >= self.patience:\n",
    "                print(\n",
    "                    f\"Validation loss hasn't improved for {self.patience} epochs. Stopping training early.\"\n",
    "                )\n",
    "                break\n",
    "\n",
    "        print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "351a0eb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T09:00:30.649525Z",
     "iopub.status.busy": "2025-04-28T09:00:30.649312Z",
     "iopub.status.idle": "2025-04-28T09:00:30.654032Z",
     "shell.execute_reply": "2025-04-28T09:00:30.653512Z"
    },
    "papermill": {
     "duration": 0.008984,
     "end_time": "2025-04-28T09:00:30.654955",
     "exception": false,
     "start_time": "2025-04-28T09:00:30.645971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class EmbeddingHead(torch.nn.Module):\n",
    "    def __init__(self, in_features=768, embedding_dim=512, normalize=True, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.normalize_output = normalize\n",
    "        self.embedding = torch.nn.Linear(in_features, embedding_dim)\n",
    "        self.bn = torch.nn.BatchNorm1d(embedding_dim)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.dropout(x)\n",
    "        if self.normalize_output:\n",
    "            x = torch.nn.functional.normalize(x, p=2, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f38570dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T09:00:30.660969Z",
     "iopub.status.busy": "2025-04-28T09:00:30.660790Z",
     "iopub.status.idle": "2025-04-28T09:00:30.672941Z",
     "shell.execute_reply": "2025-04-28T09:00:30.672432Z"
    },
    "papermill": {
     "duration": 0.016365,
     "end_time": "2025-04-28T09:00:30.673868",
     "exception": false,
     "start_time": "2025-04-28T09:00:30.657503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NTXentLoss(torch.nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        \"\"\"\n",
    "        z_i: (batch_size, embedding_dim) - Embedding of original images\n",
    "        z_j: (batch_size, embedding_dim) - Embedding of posittive images\n",
    "        \"\"\"\n",
    "        batch_size = z_i.shape[0]\n",
    "\n",
    "        # Normalize the embeddings\n",
    "        z_i = F.normalize(z_i, dim=1)\n",
    "        z_j = F.normalize(z_j, dim=1)\n",
    "\n",
    "        # Concatenate embeddings to form a (2N, embedding_dim) tensor\n",
    "        z = torch.cat([z_i, z_j], dim=0)\n",
    "\n",
    "        # Compute cosine similarity matrix (scaled by temperature)\n",
    "        sim = torch.matmul(z, z.T) / self.temperature  # shape: (2N, 2N)\n",
    "\n",
    "        # Mask out self-similarities by replacing diagonal with a very low value\n",
    "        mask = torch.eye(2 * batch_size, dtype=torch.bool, device=z.device)\n",
    "        sim.masked_fill_(mask, -1e9)\n",
    "\n",
    "        # For each sample i, the positive sample is at index (i + batch_size) mod (2N)\n",
    "        positive_indices = (\n",
    "            torch.arange(2 * batch_size, device=z.device) + batch_size\n",
    "        ) % (2 * batch_size)\n",
    "        positives = sim[torch.arange(2 * batch_size), positive_indices].unsqueeze(1)\n",
    "\n",
    "        # Remove the positive column from sim to obtain negatives for each sample.\n",
    "        all_indices = (\n",
    "            torch.arange(2 * batch_size, device=z.device)\n",
    "            .unsqueeze(0)\n",
    "            .expand(2 * batch_size, -1)\n",
    "        )\n",
    "        pos_indices = positive_indices.unsqueeze(1)\n",
    "        neg_mask = all_indices != pos_indices\n",
    "        negatives = sim[neg_mask].view(2 * batch_size, -1)\n",
    "\n",
    "        # Construct logits: first column is the positive, remaining are negatives.\n",
    "        logits = torch.cat([positives, negatives], dim=1)\n",
    "\n",
    "        # Labels: the positive is at index 0 for each sample.\n",
    "        labels = torch.zeros(2 * batch_size, dtype=torch.long, device=z.device)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class NTXentLossHardNegatives(torch.nn.Module):\n",
    "    def __init__(self, temperature=0.5, top_k_negatives=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            temperature: Scaling factor for similarities.\n",
    "            top_k_negatives (int, optional): If not None, use only the top K\n",
    "                                             hardest negatives per anchor.\n",
    "                                             Defaults to None (use all negatives).\n",
    "        \"\"\"\n",
    "        super(NTXentLossHardNegatives, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.top_k_negatives = top_k_negatives\n",
    "        # Sá»­ dá»¥ng giÃ¡ trá»‹ Ã¢m vÃ´ cÃ¹ng nhá» Ä‘á»ƒ Ä‘áº£m báº£o khÃ´ng Ä‘Æ°á»£c chá»n bá»Ÿi topk\n",
    "        self.mask_value = -float('inf')\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        \"\"\"\n",
    "        z_i: (batch_size, embedding_dim) - Embedding of product images (or reviews)\n",
    "        z_j: (batch_size, embedding_dim) - Embedding of corresponding review images (or products)\n",
    "        \"\"\"\n",
    "        device = z_i.device\n",
    "        batch_size = z_i.shape[0]\n",
    "\n",
    "        # 1. Normalize the embeddings\n",
    "        z_i = F.normalize(z_i, dim=1)\n",
    "        z_j = F.normalize(z_j, dim=1)\n",
    "\n",
    "        # 2. Concatenate embeddings: [p1..pn, r1..rn]\n",
    "        z = torch.cat([z_i, z_j], dim=0) # shape: (2N, embedding_dim)\n",
    "        n_samples = z.shape[0] # Should be 2 * batch_size\n",
    "\n",
    "        # 3. Compute cosine similarity matrix (scaled by temperature)\n",
    "        sim = torch.matmul(z, z.T) / self.temperature  # shape: (2N, 2N)\n",
    "\n",
    "        # 4. Create masks\n",
    "        # Mask for self-similarities (diagonal)\n",
    "        self_mask = torch.eye(n_samples, dtype=torch.bool, device=device)\n",
    "\n",
    "        # Mask for positive pairs\n",
    "        # Positive for pi (at index i) is ri (at index i + batch_size)\n",
    "        # Positive for ri (at index i + batch_size) is pi (at index i)\n",
    "        pos_indices = (torch.arange(n_samples, device=device) + batch_size) % n_samples\n",
    "        # Create a full mask for positives for easier indexing later\n",
    "        pos_mask = torch.zeros_like(self_mask, dtype=torch.bool)\n",
    "        pos_mask[torch.arange(n_samples), pos_indices] = True\n",
    "\n",
    "        # 5. Extract positive similarities\n",
    "        # Use the pos_indices calculated earlier\n",
    "        positives = sim[torch.arange(n_samples), pos_indices].unsqueeze(1) # shape: (2N, 1)\n",
    "\n",
    "        # 6. Extract negative similarities and apply hard negative mining\n",
    "        # Start with the full similarity matrix\n",
    "        negatives_sim = sim.clone()\n",
    "\n",
    "        # Mask out self-similarities and positives so they aren't selected as negatives\n",
    "        negatives_sim.masked_fill_(self_mask, self.mask_value)\n",
    "        negatives_sim.masked_fill_(pos_mask, self.mask_value)\n",
    "\n",
    "        if self.top_k_negatives is not None:\n",
    "            # Ensure k is not larger than the number of actual negatives available\n",
    "            num_actual_negatives = n_samples - 2 # Exclude self and positive\n",
    "            k_to_use = min(self.top_k_negatives, num_actual_negatives)\n",
    "\n",
    "            if k_to_use > 0:\n",
    "                # Select the top k highest similarity values (hardest negatives) for each row\n",
    "                # topk returns values and indices, we only need values\n",
    "                hard_negatives, _ = torch.topk(negatives_sim, k_to_use, dim=1, largest=True)\n",
    "            else:\n",
    "                # Handle edge case where k=0 or no negatives available (e.g., batch_size=1)\n",
    "                hard_negatives = torch.empty((n_samples, 0), device=device)\n",
    "\n",
    "            negatives = hard_negatives # Use only the hardest ones\n",
    "        else:\n",
    "            # If top_k_negatives is None, use all available negatives\n",
    "            # We can extract them using the combined mask\n",
    "            # This branch makes it equivalent to the original NTXentLoss logic,\n",
    "            # but extracting via topk might be computationally similar anyway.\n",
    "            # Let's extract them directly for clarity when not using topk.\n",
    "            negative_mask = ~(self_mask | pos_mask)\n",
    "            # Need to gather carefully, perhaps sticking with topk is simpler?\n",
    "            # Let's stick to topk logic for consistency, setting k to max possible negatives.\n",
    "            num_actual_negatives = n_samples - 2\n",
    "            if num_actual_negatives > 0:\n",
    "                negatives, _ = torch.topk(negatives_sim, num_actual_negatives, dim=1, largest=True)\n",
    "            else:\n",
    "                 negatives = torch.empty((n_samples, 0), device=device)\n",
    "\n",
    "\n",
    "        # 7. Construct logits\n",
    "        # First column is the positive similarity, subsequent columns are negative similarities\n",
    "        logits = torch.cat([positives, negatives], dim=1) # shape: (2N, 1 + k_to_use or 1 + 2N-2)\n",
    "\n",
    "        # 8. Create labels\n",
    "        # The positive similarity is always at index 0\n",
    "        labels = torch.zeros(n_samples, dtype=torch.long, device=device)\n",
    "\n",
    "        # 9. Calculate cross-entropy loss\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12b7e63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T09:00:30.679833Z",
     "iopub.status.busy": "2025-04-28T09:00:30.679632Z",
     "iopub.status.idle": "2025-04-28T09:08:47.774592Z",
     "shell.execute_reply": "2025-04-28T09:08:47.773864Z"
    },
    "papermill": {
     "duration": 497.099598,
     "end_time": "2025-04-28T09:08:47.776030",
     "exception": false,
     "start_time": "2025-04-28T09:00:30.676432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/swin_v2_t-b137f0e2.pth\" to /root/.cache/torch/hub/checkpoints/swin_v2_t-b137f0e2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|          | 0.00/109M [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  3%|â–Ž         | 3.12M/109M [00:00<00:03, 32.7MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 15%|â–ˆâ–Œ        | 16.5M/109M [00:00<00:01, 83.4MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 32.9M/109M [00:00<00:00, 101MB/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 49.1M/109M [00:00<00:00, 124MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 61.1M/109M [00:00<00:00, 76.4MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 70.4M/109M [00:00<00:00, 68.9MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 82.0M/109M [00:01<00:00, 69.7MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 98.4M/109M [00:01<00:00, 84.4MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 109M/109M [00:01<00:00, 87.6MB/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load train datasets!\n",
      "/kaggle/input/review-thesis-datasets/part7/part7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/review-thesis-datasets/part12/part12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/review-thesis-datasets/part1/part1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/review-thesis-datasets/part10/part10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/review-thesis-datasets/part6/part6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/review-thesis-datasets/part13/part13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/review-thesis-datasets/part2/part2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/review-thesis-datasets/part11/part11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/review-thesis-datasets/part8/part8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/review-thesis-datasets/part3/part3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/review-thesis-datasets/part4/part4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/review-thesis-datasets/part5/part5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load val datasets!\n",
      "/kaggle/input/val-review-thesis-datasets/part9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: torch.Size([64, 3, 224, 224]), Labels: torch.Size([64, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: torch.Size([64, 3, 224, 224]), Labels: torch.Size([64, 3, 224, 224]), Negative: torch.Size([64, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import torch\n",
    "from torchvision.models import swin_v2_t, Swin_V2_T_Weights\n",
    "\n",
    "import wandb\n",
    "\n",
    "data_path = \"/kaggle/input/review-thesis-datasets\"\n",
    "val_data_path = \"/kaggle/input/val-review-thesis-datasets\"\n",
    "# Hyperparameters\n",
    "base_lr = 2e-5  # Learning rate ban Ä‘áº§u\n",
    "num_epochs = 25\n",
    "dataset_size = -1\n",
    "val_dataset_size = -1\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1  # 10% epochs Ä‘áº§u lÃ  warmup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = swin_v2_t(weights=Swin_V2_T_Weights.IMAGENET1K_V1)\n",
    "model.head = EmbeddingHead(in_features=768, embedding_dim=256)\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "old_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),  # Resize táº¥t cáº£ áº£nh vá» 224x224\n",
    "        transforms.ToTensor(),  # Chuyá»ƒn áº£nh thÃ nh tensor\n",
    "        transforms.RandomHorizontalFlip(p=0.5),  # Láº­t ngang áº£nh vá»›i xÃ¡c suáº¥t 50%\n",
    "        transforms.ColorJitter(brightness=0.3),  # Äiá»u chá»‰nh Ä‘á»™ sÃ¡ng (Â±30%)\n",
    "        transforms.RandomPerspective(\n",
    "            distortion_scale=0.5, p=0.5\n",
    "        ),  # Biáº¿n dáº¡ng phá»‘i cáº£nh\n",
    "        transforms.RandomRotation(degrees=30),  # Xoay áº£nh trong khoáº£ng Â±30 Ä‘á»™\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),  # Resize táº¥t cáº£ áº£nh vá» 224x224\n",
    "        transforms.RandomApply([\n",
    "            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)\n",
    "        ], p=0.4),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.RandomApply([\n",
    "            transforms.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0)) # Kernel size needs to be odd\n",
    "        ], p=0.4),\n",
    "        transforms.ToTensor(),  # Chuyá»ƒn áº£nh thÃ nh tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),  # Resize táº¥t cáº£ áº£nh vá» 224x224\n",
    "        transforms.ToTensor(),  # Chuyá»ƒn áº£nh thÃ nh tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print('Load train datasets!')\n",
    "dataset = TripletDataset(data_path, transform=transform, limit=dataset_size)\n",
    "print('Load val datasets!')\n",
    "val_dataset = EvalTripletDataset(val_data_path, transform=val_transform, limit=val_dataset_size, type='val')\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Khá»Ÿi táº¡o Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=base_lr)\n",
    "criterion = NTXentLossHardNegatives(temperature=0.25, top_k_negatives=64)\n",
    "lr_scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=base_lr,\n",
    "    epochs=num_epochs,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=warmup_ratio\n",
    ")\n",
    "\n",
    "# Láº·p qua dataloader Ä‘á»ƒ láº¥y batch\n",
    "for images, labels in train_loader:\n",
    "    print(\n",
    "        f\"Batch size: {images.shape}, Labels: {labels.shape}\"\n",
    "    )\n",
    "    break  # Dá»«ng sau batch Ä‘áº§u tiÃªn\n",
    "\n",
    "for images, labels, negative in val_loader:\n",
    "    print(\n",
    "        f\"Batch size: {images.shape}, Labels: {labels.shape}, Negative: {negative.shape}\"\n",
    "    )\n",
    "    break  # Dá»«ng sau batch Ä‘áº§u tiÃªn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95793535",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T09:08:47.785287Z",
     "iopub.status.busy": "2025-04-28T09:08:47.784791Z",
     "iopub.status.idle": "2025-04-28T09:08:47.789437Z",
     "shell.execute_reply": "2025-04-28T09:08:47.788811Z"
    },
    "papermill": {
     "duration": 0.010192,
     "end_time": "2025-04-28T09:08:47.790464",
     "exception": false,
     "start_time": "2025-04-28T09:08:47.780272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787 66\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader), len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c93ec4c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T09:08:47.798540Z",
     "iopub.status.busy": "2025-04-28T09:08:47.798327Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-04-28T09:08:47.794229",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlong02042003\u001b[0m (\u001b[33mmosque\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250428_090847-ahj4ksll\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrue_18_NTXentLoss_hardnegative_swinv2\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mosque/review%20thesis%20project\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/mosque/review%20thesis%20project/runs/ahj4ksll\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (122544900 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (135435040 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Train Loss: 3.6137, Val Loss: 2.9972\n",
      "New best validation loss: 2.9972. Saving checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (122544900 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (135435040 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/25], Train Loss: 3.2086, Val Loss: 2.7372\n",
      "New best validation loss: 2.7372. Saving checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (122544900 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (135435040 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/25], Train Loss: 3.0115, Val Loss: 2.6426\n",
      "New best validation loss: 2.6426. Saving checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (122544900 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (135435040 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25], Train Loss: 2.9068, Val Loss: 2.5838\n",
      "New best validation loss: 2.5838. Saving checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (122544900 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (135435040 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/25], Train Loss: 2.8318, Val Loss: 2.5473\n",
      "New best validation loss: 2.5473. Saving checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (135435040 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (122544900 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/25], Train Loss: 2.7794, Val Loss: 2.5140\n",
      "New best validation loss: 2.5140. Saving checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (122544900 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (135435040 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/25], Train Loss: 2.7303, Val Loss: 2.4917\n",
      "New best validation loss: 2.4917. Saving checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (135435040 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (122544900 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/25], Train Loss: 2.6882, Val Loss: 2.4760\n",
      "New best validation loss: 2.4760. Saving checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (122544900 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (135435040 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/25], Train Loss: 2.6526, Val Loss: 2.4590\n",
      "New best validation loss: 2.4590. Saving checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (122544900 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (135435040 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/25], Train Loss: 2.6187, Val Loss: 2.4490\n",
      "New best validation loss: 2.4490. Saving checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (135435040 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (122544900 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/25], Train Loss: 2.5921, Val Loss: 2.4378\n",
      "New best validation loss: 2.4378. Saving checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (135435040 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (122544900 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/25], Train Loss: 2.5669, Val Loss: 2.4360\n",
      "New best validation loss: 2.4360. Saving checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (122544900 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (135435040 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/25], Train Loss: 2.5411, Val Loss: 2.4297\n",
      "New best validation loss: 2.4297. Saving checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (135435040 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (122544900 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/25], Train Loss: 2.5210, Val Loss: 2.4261\n",
      "New best validation loss: 2.4261. Saving checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (122544900 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (135435040 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/25], Train Loss: 2.5026, Val Loss: 2.4212\n",
      "New best validation loss: 2.4212. Saving checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (122544900 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (135435040 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/25], Train Loss: 2.4873, Val Loss: 2.4165\n",
      "New best validation loss: 2.4165. Saving checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (122544900 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (135435040 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/25], Train Loss: 2.4756, Val Loss: 2.4156\n",
      "New best validation loss: 2.4156. Saving checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (122544900 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (135435040 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/25], Train Loss: 2.4628, Val Loss: 2.4115\n",
      "New best validation loss: 2.4115. Saving checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (122544900 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (135435040 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/25], Train Loss: 2.4534, Val Loss: 2.4106\n",
      "New best validation loss: 2.4106. Saving checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (135435040 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (122544900 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/25], Train Loss: 2.4481, Val Loss: 2.4080\n",
      "New best validation loss: 2.4080. Saving checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (122544900 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (135435040 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "wandb.login(key=\"c436a4917c43e09e30b67b919bd06e7bf7b0c10d\")\n",
    "wandb.init(\n",
    "    project=\"review thesis project\",\n",
    "    name=\"true_18_NTXentLoss_hardnegative_swinv2\",\n",
    "    # id=\"r2k837mu\",\n",
    "    # resume=\"allow\"\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "trainer = SpamDetectorTrainer(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=lr_scheduler,\n",
    "    device=device,\n",
    "    epochs=num_epochs,\n",
    "    log_writer=wandb,\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7000694,
     "sourceId": 11211479,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6998594,
     "sourceId": 11312905,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7074935,
     "sourceId": 11522491,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-28T09:00:15.195344",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
